{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cxDTVNx9QBXv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "w1_0fVjmQBX0"
   },
   "outputs": [],
   "source": [
    "train_path = '../data/20news-bydate-train'\n",
    "test_path = '../data/20news-bydate-test'\n",
    "saved_path = '../data/'\n",
    "MAX_SENTENCE_LENGTH = 500\n",
    "padding_ID = 0\n",
    "unknown_ID = 1\n",
    "NUM_OF_CLASSES = 20\n",
    "EMBEDDINGS_SIZE = 300\n",
    "LSTM_HIDDEN_SIZE = 50\n",
    "batch_size = 50\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HMoS_IhQBX1"
   },
   "outputs": [],
   "source": [
    "#generate data and vocabulary\n",
    "def get_data_and_vocab():\n",
    "    def collect_data_from(parent_path, newsgroup_list, word_count=None):\n",
    "        data = []\n",
    "        for id, newsgroup in enumerate(newsgroup_list):\n",
    "            path = parent_path+'/'+newsgroup+'/'\n",
    "            files = [(file_name, path+file_name)\n",
    "                     for file_name in os.listdir(path)]\n",
    "            print('Processing: {}-{}'.format(id, newsgroup))\n",
    "            for file_name, file_path in files:\n",
    "                with open(file_path) as f:\n",
    "                    text = f.read().lower()\n",
    "                    words = re.split('\\W+', text)\n",
    "                    if word_count is not None:\n",
    "                        for word in words:\n",
    "                            if word in word_count:\n",
    "                                word_count[word] += 1\n",
    "                            else:\n",
    "                                word_count[word] = 0\n",
    "                    content = ' '.join(words)\n",
    "                    assert len(content.splitlines()) == 1\n",
    "                    data.append(str(id)+'<fff>'+file_name+'<fff>'+content)\n",
    "        return data\n",
    "    word_count = dict()\n",
    "    newsgroup_list = [newsgroup for newsgroup in os.listdir(train_path)]\n",
    "    newsgroup_list.sort()\n",
    "    train_data = collect_data_from(train_path, newsgroup_list, word_count)\n",
    "    vocab = [word for word, freq in word_count.items() if freq > 10]\n",
    "    vocab.sort()\n",
    "    with open(saved_path+'vocab-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(vocab))\n",
    "    newsgroup_list = [newsgroup for newsgroup in os.listdir(test_path)]\n",
    "    test_data = collect_data_from(test_path, newsgroup_list)\n",
    "    with open(saved_path+'20news-train-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(train_data))\n",
    "    with open(saved_path+'20news-test-raw.txt', 'w') as f:\n",
    "        f.write('\\n'.join(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKHuqPtAQBX2"
   },
   "outputs": [],
   "source": [
    "def encode_data(data_path, vocab_path):\n",
    "    with open(vocab_path) as f:\n",
    "        vocab = dict([(word, word_ID+2)\n",
    "                     for word_ID, word in enumerate(f.read().splitlines())])\n",
    "    with open(data_path) as f:\n",
    "        documents = [(line.split('<fff>')) for line in f.read().splitlines()]\n",
    "    encoded_data = []\n",
    "    for document in documents:\n",
    "        label, doc_id, text = document\n",
    "        words = text.split()[:MAX_SENTENCE_LENGTH]\n",
    "        sentence_length = len(words)\n",
    "        encoded_text = []\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                encoded_text.append(str(vocab[word]))\n",
    "            else:\n",
    "                encoded_text.append(str(unknown_ID))\n",
    "        for i in range(sentence_length, MAX_SENTENCE_LENGTH):\n",
    "            encoded_text.append(str(padding_ID))\n",
    "        encoded_data.append(str(label)+'<fff>'+str(doc_id)+'<fff>' +\n",
    "                            str(sentence_length)+'<fff>'+' '.join(encoded_text))\n",
    "    dir_name = '/'.join(data_path.split('/')[:-1])\n",
    "    file_name = '-'.join(data_path.split('/')\n",
    "                         [-1].split('-')[:-1])+'-encoded.txt'\n",
    "    with open(dir_name+'/'+file_name, 'w') as f:\n",
    "        f.write('\\n'.join(encoded_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Doze_rGrQBX4",
    "outputId": "8c9c0146-26b0-4b26-ea6a-15954e362bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n",
      "Processing: 0-alt.atheism\n",
      "Processing: 1-comp.graphics\n",
      "Processing: 2-comp.os.ms-windows.misc\n",
      "Processing: 3-comp.sys.ibm.pc.hardware\n",
      "Processing: 4-comp.sys.mac.hardware\n",
      "Processing: 5-comp.windows.x\n",
      "Processing: 6-misc.forsale\n",
      "Processing: 7-rec.autos\n",
      "Processing: 8-rec.motorcycles\n",
      "Processing: 9-rec.sport.baseball\n",
      "Processing: 10-rec.sport.hockey\n",
      "Processing: 11-sci.crypt\n",
      "Processing: 12-sci.electronics\n",
      "Processing: 13-sci.med\n",
      "Processing: 14-sci.space\n",
      "Processing: 15-soc.religion.christian\n",
      "Processing: 16-talk.politics.guns\n",
      "Processing: 17-talk.politics.mideast\n",
      "Processing: 18-talk.politics.misc\n",
      "Processing: 19-talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "get_data_and_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5cqSVEFQBX5"
   },
   "outputs": [],
   "source": [
    "encode_data(saved_path+'20news-train-raw.txt',saved_path+'vocab-raw.txt')\n",
    "encode_data(saved_path+'20news-test-raw.txt',saved_path+'vocab-raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "s02hruZKQBX6"
   },
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, LSTM_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.LSTM_size = LSTM_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0) #Embeddings Layer\n",
    "        self.LSTM = nn.LSTM(embedding_size, LSTM_size, batch_first=True) #LSTM layer\n",
    "        self.linear = nn.Linear(LSTM_size, NUM_OF_CLASSES) #Linear layer\n",
    "        self.softmax = nn.Softmax() #softmax layer\n",
    "\n",
    "    def forward(self, x, mask, length):\n",
    "        embeddings_text = self.embeddings(x) #(batch_size, seq_length, embeddings_size)\n",
    "        lstm_output, (ht, ct) = self.LSTM(embeddings_text) #(batch_size, seq_length, lstm_hidden_size)\n",
    "        lstm_output = mask*lstm_output #mul with mask (batch_size, seq_length, lstm_hidden_size)\n",
    "        lstm_output = torch.sum(lstm_output, dim=1)/length # average (batch_size,lstm_hidden_size)\n",
    "        output = self.linear(lstm_output) #(batch_size, NUM_OF_CLASSES)\n",
    "        return self.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CRG6g78_QBX7"
   },
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, labels, encoded_data, data_length):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.encoded_data = encoded_data\n",
    "        self.data_length = data_length\n",
    "    def __len__(self):\n",
    "        return self.encoded_data.size(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_data[index], self.labels[index], self.data_length[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cUKzNHE4QBX8"
   },
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    encoded_data = []\n",
    "    labels = []\n",
    "    sentence_length = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    for line in lines:\n",
    "        label, doc_id, sent_length, text = line.split('<fff>')\n",
    "        labels.append(int(label))\n",
    "        encoded_text = [int(u) for u in text.split(' ')]\n",
    "        encoded_data.append(encoded_text)\n",
    "        sentence_length.append(int(sent_length))\n",
    "    return torch.tensor(encoded_data), torch.tensor(labels), torch.tensor(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Yc3z2t7gQBX8"
   },
   "outputs": [],
   "source": [
    "#Load data\n",
    "train_data, train_labels, train_length = load_data(saved_path+'20news-train-encoded.txt')\n",
    "test_data, test_labels, test_length = load_data(saved_path+'20news-test-encoded.txt')\n",
    "with open(saved_path+'vocab-raw.txt', 'r',encoding='iso 8859-15') as f:\n",
    "    vocab_size = len(f.read().splitlines())+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Nc7iK3G2QBX9"
   },
   "outputs": [],
   "source": [
    "# Generate data, model and optimizer\n",
    "train_set = Data(train_labels, train_data, train_length)\n",
    "valid_set = Data(test_labels[:1000], test_data[:1000], test_length[:1000])\n",
    "model = LSTM_Model(vocab_size, EMBEDDINGS_SIZE, LSTM_HIDDEN_SIZE)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "it = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QX_hXy1dSFZ-"
   },
   "outputs": [],
   "source": [
    "def validation(model, optimizer, data, criterion):\n",
    "  valid_loader = DataLoader(data, batch_size=batch_size)\n",
    "  total_loss = 0.0\n",
    "  num_sample = 0\n",
    "  for data, labels, length in valid_loader:\n",
    "    mask = (data!=0).unsqueeze(2) #generate sentence mask\n",
    "    length = length.unsqueeze(1)\n",
    "    predict = model.forward(data, mask, length)\n",
    "    loss = criterion(predict, labels)\n",
    "    total_loss+= loss\n",
    "    num_sample+=1\n",
    "  return total_loss/num_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HBAi-YmiQBX9",
    "outputId": "1a0b142a-873b-4064-c133-d6c84c6d3561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 600:\n",
      "Train loss: 2.276887\n",
      "Val loss: 2.687359\n",
      "Iteration 700:\n",
      "Train loss: 2.306279\n",
      "Val loss: 2.623698\n",
      "Epoch 1: \n",
      "Iteration 800:\n",
      "Train loss: 2.277609\n",
      "Val loss: 2.675293\n",
      "Iteration 900:\n",
      "Train loss: 2.258175\n",
      "Val loss: 2.644224\n",
      "Epoch 2: \n",
      "Iteration 1000:\n",
      "Train loss: 2.257921\n",
      "Val loss: 2.641708\n",
      "Iteration 1100:\n",
      "Train loss: 2.228795\n",
      "Val loss: 2.506616\n",
      "Epoch 3: \n",
      "Iteration 1200:\n",
      "Train loss: 2.218322\n",
      "Val loss: 2.469356\n",
      "Iteration 1300:\n",
      "Train loss: 2.200508\n",
      "Val loss: 2.462802\n",
      "Iteration 1400:\n",
      "Train loss: 2.197955\n",
      "Val loss: 2.453601\n",
      "Epoch 4: \n",
      "Iteration 1500:\n",
      "Train loss: 2.182336\n",
      "Val loss: 2.414129\n",
      "Iteration 1600:\n",
      "Train loss: 2.181095\n",
      "Val loss: 2.439703\n",
      "Epoch 5: \n",
      "Iteration 1700:\n",
      "Train loss: 2.183646\n",
      "Val loss: 2.419493\n",
      "Iteration 1800:\n",
      "Train loss: 2.168574\n",
      "Val loss: 2.429601\n",
      "Epoch 6: \n",
      "Iteration 1900:\n",
      "Train loss: 2.168563\n",
      "Val loss: 2.462502\n",
      "Iteration 2000:\n",
      "Train loss: 2.140737\n",
      "Val loss: 2.475493\n",
      "Epoch 7: \n",
      "Iteration 2100:\n",
      "Train loss: 2.152933\n",
      "Val loss: 2.478423\n",
      "Iteration 2200:\n",
      "Train loss: 2.137226\n",
      "Val loss: 2.466168\n",
      "Iteration 2300:\n",
      "Train loss: 2.142182\n",
      "Val loss: 2.481287\n",
      "Epoch 8: \n",
      "Iteration 2400:\n",
      "Train loss: 2.130994\n",
      "Val loss: 2.475141\n",
      "Iteration 2500:\n",
      "Train loss: 2.136914\n",
      "Val loss: 2.431019\n",
      "Epoch 9: \n",
      "Iteration 2600:\n",
      "Train loss: 2.133403\n",
      "Val loss: 2.426378\n",
      "Iteration 2700:\n",
      "Train loss: 2.134522\n",
      "Val loss: 2.442407\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0.0\n",
    "for epoch in range(10):\n",
    "    print('Epoch %d: ' % epoch)\n",
    "    for data, labels, length in train_loader:\n",
    "        it += 1\n",
    "        mask = (data!=0).unsqueeze(2) #generate sentence mask\n",
    "        length = length.unsqueeze(1)\n",
    "        predict = model.forward(data, mask, length)\n",
    "        loss = criterion(predict, labels)\n",
    "        total_loss+= loss\n",
    "        if it % 100 == 0:\n",
    "            print('Iteration %d:\\nTrain loss: %f\\nVal loss: %f' % (it, total_loss/100, validation(model, optimizer, valid_set, criterion)))\n",
    "            total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "QwA3BO3YQBX-"
   },
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()},\n",
    "    f'LSTM_model_batch_size_{batch_size}_LSTM_{LSTM_HIDDEN_SIZE}_Embeddings_{EMBEDDINGS_SIZE}_lr_{learning_rate}.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "AM_BdE8GQBX_"
   },
   "outputs": [],
   "source": [
    "#Load model for test\n",
    "u = torch.load(f'LSTM_model_batch_size_{batch_size}_LSTM_{LSTM_HIDDEN_SIZE}_Embeddings_{EMBEDDINGS_SIZE}_lr_{learning_rate}.pth')\n",
    "model = LSTM_Model(vocab_size, EMBEDDINGS_SIZE, LSTM_HIDDEN_SIZE)\n",
    "model.load_state_dict(u['model_state_dict'])\n",
    "optimizer.load_state_dict(u['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ut7oLLWYQBX_",
    "outputId": "8a71b304-5b61-47eb-a14b-01ab99c95ab8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.6988847583643123\n"
     ]
    }
   ],
   "source": [
    "test_set = Data(test_labels, test_data, test_length) #Load test data\n",
    "test_loader = DataLoader(test_set, batch_size=50)\n",
    "num_true_preds = 0\n",
    "for data, labels, length in test_loader:\n",
    "    mask = (data!=0).unsqueeze(2)\n",
    "    length = length.unsqueeze(1)\n",
    "    test_plabels_eval = model.forward(data, mask, length)\n",
    "    s_labels = torch.argmax(test_plabels_eval, axis=1)\n",
    "    num_true_preds += float(torch.sum(s_labels == labels))\n",
    "print('Accuracy on test data: ', num_true_preds/test_set.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7RnyEpsczUQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
